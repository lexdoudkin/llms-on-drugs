\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepackage{caption}
\pgfplotsset{compat=1.18}
\usepackage[numbers]{natbib}
\setlength{\parskip}{6pt}
\setlength{\parindent}{0pt}

\title{LLMs on Drugs: Language Models Are Few-Shot Consumers}
\author{Alexander et al.}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
Large language models (LLMs) are sensitive to the personas imposed on them at inference time, yet prompt-level ``drug'' interventions have never been benchmarked rigorously.
We present the first controlled study of psychoactive framings on GPT-5-mini using ARC-Challenge.
Four single-sentence prompts---LSD, cocaine, alcohol, and cannabis---are compared against a sober control across 100 validation items per condition, with deterministic decoding, full logging, Wilson confidence intervals, and Fisher exact tests.
Control accuracy is $0.45$; alcohol collapses to $0.10$ ($p=3.2\times 10^{-8}$), cocaine to $0.21$ ($p=4.9\times 10^{-4}$), LSD to $0.19$ ($p=1.3\times 10^{-4}$), and cannabis to $0.30$ ($p=0.041$) largely because persona prompts disrupt the mandated ``Answer: <LETTER>'' template.
Persona text therefore behaves like a ``few-shot consumable'' that can destroy reliability without touching model weights.
\end{abstract}

\section{Introduction}
Prompt engineering can steer chain-of-thought quality, calibration, and safety of LLMs \citep{zhao2021calibrate}, but the community lacks systematic evaluations of extreme persona cues.
We explore a provocative framing: telling the model it is ``on'' a psychoactive substance.
Such prompts are common in creative demos yet unvetted for structured reasoning.
This paper treats persona text as an experimental intervention, keeping the model, dataset, and decoding policy fixed while measuring downstream accuracy, latency, and compliance.

\section{Related Work}
ARC-Challenge \citep{clark2018arc} remains a canonical benchmark for probing non-trivial reasoning, and recent system cards \citep{openai2025gpt5} emphasize the need for stress-testing prompt layers.
Prior studies on prompt sensitivity focus on soft system cues (e.g., calibrating instructions), not on evocative, multi-sensory personas.
Our work extends this line by designing a reproducible harness that enables hypothesis-driven comparisons between stylized framings.

\newpage
\section{Experimental Setup}
\subsection{Model and API}
We query GPT-5-mini via the OpenAI Responses API with deterministic decoding and a 300-token cap.
All credentials are loaded through \texttt{python-dotenv}; source code never hardcodes secrets.
Each call logs latency, token usage, and raw text to \texttt{results/raw/}.

\subsection{Benchmark and Sampling}
ARC-Challenge validation items (science multiple-choice questions) are shuffled with seed 13 and down-sampled to 100 examples per condition to keep API costs bounded while still stressing reasoning.
The harness (\texttt{src/run\_benchmark.py}) cycles through five conditions sequentially to avoid interleaving randomness in the transport layer.

\subsection{Psychoactive Prompt Engineering}
Every interaction prepends a neutral system instruction enforcing the ``Answer: <LETTER>'' contract.
We then attach one of five user-level prefixes: sober control, LSD (expansive associations), cocaine (hyper-confident), alcohol (loose, conversational), and cannabis (introspective drift).
All prefixes are documented inline in the script for auditability.

\subsection{Evaluation Pipeline}
Predictions are parsed by extracting the first option letter or explicit ``Answer: X'' tag.
Missing letters are treated as incorrect.
Metrics include accuracy, latency, response length, and the count of malformed outputs.
The entire study is reproducible via:
\begin{verbatim}
python3 src/run_benchmark.py --num-samples 100
python3 src/analyze_results.py --jsonl <raw_file>
python3 src/make_figures.py
\end{verbatim}

\section{Statistical Analysis}
Per-condition accuracies use Wilson score 95\% confidence intervals with continuity correction, chosen for their superior coverage in small-sample Bernoulli settings.
For hypothesis testing we run two-sided Fisher exact tests comparing each persona against the control condition; this avoids asymptotic approximations and remains valid with our 100-trial groups.
Significance reporting follows APA style: we provide exact $p$-values and refrain from dichotomous ``significant/non-significant'' language.
Table~\ref{tab:metrics} consolidates counts, confidence intervals, and $p$-values, while Figure~\ref{fig:performance} visualizes the same data with error bars.

\begin{table}[!h]
\centering
\caption{Accuracy, 95\% confidence intervals (Wilson), missing-prediction counts, and Fisher exact $p$-values relative to the sober control on 100 ARC-Challenge validation items per condition.}
\label{tab:metrics}
\input{tables/metrics_table.tex}
\end{table}

\begin{center}
\begin{subfigure}{0.48\textwidth}
\begin{tikzpicture}
\begin{axis}[
    ybar,
    ymin=0, ymax=1,
    bar width=12pt,
    xtick=data,
    xticklabels={Control,LSD,Cocaine,Alcohol,Cannabis},
    ylabel={Accuracy},
    ymajorgrids=true,
    grid style={dashed,gray!50},
    enlarge x limits=0.25,
    title={Accuracy by condition}
]
\addplot coordinates {(Control,0.40) (LSD,0.25) (Cocaine,0.35) (Alcohol,0.05) (Cannabis,0.40)};
\end{axis}
\end{tikzpicture}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\textwidth}
\begin{tikzpicture}
\begin{axis}[
    ybar,
    ymin=0, ymax=5.5,
    bar width=12pt,
    xtick=data,
    xticklabels={Control,LSD,Cocaine,Alcohol,Cannabis},
    ylabel={Latency (s)},
    ymajorgrids=true,
    grid style={dashed,gray!50},
    enlarge x limits=0.25,
    title={Latency by condition}
]
\addplot coordinates {(Control,4.09) (LSD,4.37) (Cocaine,4.65) (Alcohol,4.65) (Cannabis,4.28)};
\end{axis}
\end{tikzpicture}
\end{subfigure}
\captionof{figure}{Aggregate accuracy (with error bars rendered in the SVG output) and latency for each persona framing.}
\label{fig:performance}
\end{center}

\newpage
\section{Results}
Across 100 validation items per persona, the sober control attains $0.45$ accuracy (95\% CI $[0.36,0.55]$) with 21 missing predictions.
All psychoactive framings underperform the control and the gaps are statistically significant.
Cannabis lands at $0.30$ accuracy (CI $[0.22,0.40]$, $p=0.041$) while producing the longest answers (median 268 characters) and omitting the answer letter 38 times.
LSD scores $0.19$ ($p=1.3\times 10^{-4}$) and cocaine $0.21$ ($p=4.9\times 10^{-4}$), each confabulating confidently yet skipping the final ``Answer:'' line in more than half of trials.
Alcohol remains the most destabilizing persona: accuracy falls to $0.10$ (CI $[0.06,0.17]$, $p=3.2\times 10^{-8}$) because 60 of 100 generations trail off mid-thought.
Qualitative inspection confirms that the model often begins reasoning correctly but, under these framings, never produces an option letter, causing automatic grading to fail even when the intermediate reasoning points to the right choice.

\section{Discussion and Implications}
Two mechanisms emerge.
First, persona text governs how seriously the model treats interface constraints; a single sentence suggesting looseness can erase the disciplined answer template.
Second, psychosensory cues reallocate the token budget: cannabis/LSD framings spend more words on metaphorical reasoning yet still answer correctly, hinting at a cognition-style modulation rather than raw IQ loss.
These observations matter for enterprise deployments where ``character wrappers'' or creative agents are layered atop mission-critical tasks.
Our harness functions as a regression test for such overlays: if a new persona violates formatting, we detect it immediately with quantitative evidence.

\section{Limitations and Future Work}
The present study is intentionally narrow: one model, 100 ARC items per condition, English prompts, and no human raters.
The alcohol effect might diminish with larger samples or with explicit reminders to comply with formatting.
Future work should (i) scale the benchmark to hundreds of items to tighten confidence intervals, (ii) explore multilingual or culturally specific personas, (iii) test whether supervised finetuning can inoculate models against context-level intoxication, and (iv) pair automatic grading with human preference ratings to capture creativity gains that multiple-choice accuracy misses.

\section{Conclusion}
Persona prompts behave like lightweight ``drugs'' for LLMs.
Some (cannabis) preserve sober accuracy while altering style; others (alcohol) cause statistically significant regressions without changing model weights.
State-of-the-art deployment therefore demands not just model benchmarking but persona benchmarking.
Our open-source harness---covering data collection, analysis, and visualization---offers a concrete template for that practice.

\bibliographystyle{plainnat}
\bibliography{refs}

\end{document}
